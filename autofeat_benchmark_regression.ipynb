{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_boston, load_diabetes\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "from autofeat import AutoFeatRegressor\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [\"diabetes\", \"boston\", \"concrete\", \"airfoil\", \"wine_quality\"]\n",
    "\n",
    "# same interface for loading all datasets - adapt the datapath\n",
    "# to where you've downloaded (and renamed) the datasets\n",
    "def load_regression_dataset(name, datapath=\"../datasets/regression/\"):\n",
    "    # load one of the datasets as X and y (and possibly units)\n",
    "    units = {}\n",
    "    if name == \"boston\":\n",
    "        # sklearn boston housing dataset\n",
    "        X, y = load_boston(True)\n",
    "\n",
    "    elif name == \"diabetes\":\n",
    "        # sklearn diabetes dataset\n",
    "        X, y = load_diabetes(True)\n",
    "\n",
    "    elif name == \"concrete\":\n",
    "        # https://archive.ics.uci.edu/ml/datasets/Concrete+Compressive+Strength\n",
    "        # Cement (component 1) -- quantitative -- kg in a m3 mixture -- Input Variable\n",
    "        # Blast Furnace Slag (component 2) -- quantitative -- kg in a m3 mixture -- Input Variable\n",
    "        # Fly Ash (component 3) -- quantitative -- kg in a m3 mixture -- Input Variable\n",
    "        # Water (component 4) -- quantitative -- kg in a m3 mixture -- Input Variable\n",
    "        # Superplasticizer (component 5) -- quantitative -- kg in a m3 mixture -- Input Variable\n",
    "        # Coarse Aggregate (component 6) -- quantitative -- kg in a m3 mixture -- Input Variable\n",
    "        # Fine Aggregate (component 7)    -- quantitative -- kg in a m3 mixture -- Input Variable\n",
    "        # Age -- quantitative -- Day (1~365) -- Input Variable\n",
    "        # Concrete compressive strength -- quantitative -- MPa -- Output Variable\n",
    "        df = pd.read_csv(os.path.join(datapath, \"concrete.csv\"))\n",
    "        X = df.iloc[:, :8].to_numpy()\n",
    "        y = df.iloc[:, 8].to_numpy()\n",
    "\n",
    "    elif name == \"forest_fires\":\n",
    "        # https://archive.ics.uci.edu/ml/datasets/Forest+Fires\n",
    "        # 1. X - x-axis spatial coordinate within the Montesinho park map: 1 to 9\n",
    "        # 2. Y - y-axis spatial coordinate within the Montesinho park map: 2 to 9\n",
    "        # 3. month - month of the year: 'jan' to 'dec'\n",
    "        # 4. day - day of the week: 'mon' to 'sun'\n",
    "        # 5. FFMC - FFMC index from the FWI system: 18.7 to 96.20\n",
    "        # 6. DMC - DMC index from the FWI system: 1.1 to 291.3\n",
    "        # 7. DC - DC index from the FWI system: 7.9 to 860.6\n",
    "        # 8. ISI - ISI index from the FWI system: 0.0 to 56.10\n",
    "        # 9. temp - temperature in Celsius degrees: 2.2 to 33.30\n",
    "        # 10. RH - relative humidity in %: 15.0 to 100\n",
    "        # 11. wind - wind speed in km/h: 0.40 to 9.40\n",
    "        # 12. rain - outside rain in mm/m2 : 0.0 to 6.4\n",
    "        # 13. area - the burned area of the forest (in ha): 0.00 to 1090.84\n",
    "        # (this output variable is very skewed towards 0.0, thus it may make sense to model with the logarithm transform).\n",
    "        # --> first 4 are ignored\n",
    "        df = pd.read_csv(os.path.join(datapath, \"forest_fires.csv\"))\n",
    "        X = df.iloc[:, 4:12].to_numpy()\n",
    "        y = df.iloc[:, 12].to_numpy()\n",
    "        # perform transformation as they suggested\n",
    "        y = np.log(y + 1)\n",
    "\n",
    "    elif name == \"wine_quality\":\n",
    "        # https://archive.ics.uci.edu/ml/datasets/Wine+Quality\n",
    "        # Input variables (based on physicochemical tests):\n",
    "        # 1 - fixed acidity\n",
    "        # 2 - volatile acidity\n",
    "        # 3 - citric acid\n",
    "        # 4 - residual sugar\n",
    "        # 5 - chlorides\n",
    "        # 6 - free sulfur dioxide\n",
    "        # 7 - total sulfur dioxide\n",
    "        # 8 - density\n",
    "        # 9 - pH\n",
    "        # 10 - sulphates\n",
    "        # 11 - alcohol\n",
    "        # Output variable (based on sensory data):\n",
    "        # 12 - quality (score between 0 and 10)\n",
    "        df_red = pd.read_csv(os.path.join(datapath, \"winequality-red.csv\"), sep=\";\")\n",
    "        df_white = pd.read_csv(os.path.join(datapath, \"winequality-white.csv\"), sep=\";\")\n",
    "        # add additional categorical feature for red or white\n",
    "        X = np.hstack([np.vstack([df_red.iloc[:, :-1].to_numpy(), df_white.iloc[:, :-1].to_numpy()]), np.array([[1]*len(df_red) + [0]*len(df_white)]).T])\n",
    "        y = np.hstack([df_red[\"quality\"].to_numpy(), df_white[\"quality\"].to_numpy()])\n",
    "\n",
    "    elif name == \"airfoil\":\n",
    "        # https://archive.ics.uci.edu/ml/datasets/Airfoil+Self-Noise\n",
    "        # This problem has the following inputs:\n",
    "        # 1. Frequency, in Hertz.\n",
    "        # 2. Angle of attack, in degrees.\n",
    "        # 3. Chord length, in meters.\n",
    "        # 4. Free-stream velocity, in meters per second.\n",
    "        # 5. Suction side displacement thickness, in meters.\n",
    "        # The only output is:\n",
    "        # 6. Scaled sound pressure level, in decibels.\n",
    "        units = {\"x001\": \"Hz\", \"x003\": \"m\", \"x004\": \"m/sec\", \"x005\": \"m\"}\n",
    "        df = pd.read_csv(os.path.join(datapath, \"airfoil_self_noise.tsv\"), header=None, names=[\"x1\", \"x2\", \"x3\", \"x4\", \"x5\", \"y\"], sep=\"\\t\")\n",
    "        X = df.iloc[:, :5].to_numpy()\n",
    "        y = df[\"y\"].to_numpy()\n",
    "\n",
    "    else:\n",
    "        raise RuntimeError(\"Unknown dataset %r\" % name)\n",
    "    return np.array(X, dtype=float), np.array(y, dtype=float), units\n",
    "\n",
    "def test_model(dataset, model, param_grid):\n",
    "    # load data\n",
    "    X, y, _ = load_regression_dataset(dataset)\n",
    "    # split in training and test parts\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=12)\n",
    "    if model.__class__.__name__ == \"SVR\":\n",
    "        sscaler = StandardScaler()\n",
    "        X_train = sscaler.fit_transform(X_train)\n",
    "        X_test = sscaler.transform(X_test)\n",
    "    # train model on train split incl cross-validation for parameter selection\n",
    "    gsmodel = GridSearchCV(model, param_grid, scoring='neg_mean_squared_error', cv=5)\n",
    "    gsmodel.fit(X_train, y_train)\n",
    "    print(\"best params:\", gsmodel.best_params_)\n",
    "    print(\"best score:\", gsmodel.best_score_)\n",
    "    print(\"MSE on training data:\", mean_squared_error(y_train, gsmodel.predict(X_train)))\n",
    "    print(\"MSE on test data:\", mean_squared_error(y_test, gsmodel.predict(X_test)))\n",
    "    print(\"R^2 on training data:\", r2_score(y_train, gsmodel.predict(X_train)))\n",
    "    print(\"R^2 on test data:\", r2_score(y_test, gsmodel.predict(X_test)))\n",
    "    return gsmodel.best_estimator_\n",
    "\n",
    "def test_autofeat(dataset, feateng_steps=2):\n",
    "    # load data\n",
    "    X, y, units = load_regression_dataset(dataset)\n",
    "    # split in training and test parts\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=12)\n",
    "    # run autofeat\n",
    "    afreg = AutoFeatRegressor(verbose=1, feateng_steps=feateng_steps, units=units)\n",
    "    # fit autofeat on less data, otherwise ridge reg model with xval will overfit on new features\n",
    "    X_train_tr = afreg.fit_transform(X_train, y_train)\n",
    "    X_test_tr = afreg.transform(X_test)\n",
    "    print(\"autofeat new features:\", len(afreg.new_feat_cols_))\n",
    "    print(\"autofeat MSE on training data:\", mean_squared_error(y_train, afreg.predict(X_train_tr)))\n",
    "    print(\"autofeat MSE on test data:\", mean_squared_error(y_test, afreg.predict(X_test_tr)))\n",
    "    print(\"autofeat R^2 on training data:\", r2_score(y_train, afreg.predict(X_train_tr)))\n",
    "    print(\"autofeat R^2 on test data:\", r2_score(y_test, afreg.predict(X_test_tr)))\n",
    "    # train rreg on transformed train split incl cross-validation for parameter selection\n",
    "    print(\"# Ridge Regression\")\n",
    "    rreg = Ridge()\n",
    "    param_grid = {\"alpha\": [0.00001, 0.0001, 0.001, 0.01, 0.1, 1., 2.5, 5., 10., 25., 50., 100., 250., 500., 1000., 2500., 5000., 10000., 25000., 50000., 100000.]}\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        gsmodel = GridSearchCV(rreg, param_grid, scoring='neg_mean_squared_error', cv=5)\n",
    "        gsmodel.fit(X_train_tr, y_train)\n",
    "    print(\"best params:\", gsmodel.best_params_)\n",
    "    print(\"best score:\", gsmodel.best_score_)\n",
    "    print(\"MSE on training data:\", mean_squared_error(y_train, gsmodel.predict(X_train_tr)))\n",
    "    print(\"MSE on test data:\", mean_squared_error(y_test, gsmodel.predict(X_test_tr)))\n",
    "    print(\"R^2 on training data:\", r2_score(y_train, gsmodel.predict(X_train_tr)))\n",
    "    print(\"R^2 on test data:\", r2_score(y_test, gsmodel.predict(X_test_tr)))\n",
    "    print(\"# Random Forest\")\n",
    "    rforest = RandomForestRegressor(n_estimators=100, random_state=13)\n",
    "    param_grid = {\"min_samples_leaf\": [0.0001, 0.001, 0.01, 0.05, 0.1, 0.2]}\n",
    "    gsmodel = GridSearchCV(rforest, param_grid, scoring='neg_mean_squared_error', cv=5)\n",
    "    gsmodel.fit(X_train_tr, y_train)\n",
    "    print(\"best params:\", gsmodel.best_params_)\n",
    "    print(\"best score:\", gsmodel.best_score_)\n",
    "    print(\"MSE on training data:\", mean_squared_error(y_train, gsmodel.predict(X_train_tr)))\n",
    "    print(\"MSE on test data:\", mean_squared_error(y_test, gsmodel.predict(X_test_tr)))\n",
    "    print(\"R^2 on training data:\", r2_score(y_train, gsmodel.predict(X_train_tr)))\n",
    "    print(\"R^2 on test data:\", r2_score(y_test, gsmodel.predict(X_test_tr)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### diabetes\n",
      "(442, 10)\n",
      "#### boston\n",
      "(506, 13)\n",
      "#### concrete\n",
      "(1030, 8)\n",
      "#### airfoil\n",
      "(1503, 5)\n",
      "#### wine_quality\n",
      "(6497, 12)\n"
     ]
    }
   ],
   "source": [
    "for dsname in datasets:\n",
    "    print(\"####\", dsname)\n",
    "    X, y, _ = load_regression_dataset(dsname)\n",
    "    print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### diabetes\n",
      "best params: {'alpha': 0.01}\n",
      "best score: -3043.1448766877706\n",
      "MSE on training data: 2817.5756461735427\n",
      "MSE on test data: 3119.632550355442\n",
      "R^2 on training data: 0.541317737800587\n",
      "R^2 on test data: 0.38300930348673157\n",
      "#### boston\n",
      "best params: {'alpha': 1e-05}\n",
      "best score: -25.427148426837697\n",
      "MSE on training data: 22.4278718761592\n",
      "MSE on test data: 20.55805030529219\n",
      "R^2 on training data: 0.7361592384229154\n",
      "R^2 on test data: 0.7484031841564716\n",
      "#### concrete\n",
      "best params: {'alpha': 10000.0}\n",
      "best score: -110.34480414200303\n",
      "MSE on training data: 107.00865107837936\n",
      "MSE on test data: 110.56229503996859\n",
      "R^2 on training data: 0.6245955930727385\n",
      "R^2 on test data: 0.5643057266127827\n",
      "#### airfoil\n",
      "best params: {'alpha': 0.001}\n",
      "best score: -22.960476312553066\n",
      "MSE on training data: 22.6317043193984\n",
      "MSE on test data: 24.732769352718226\n",
      "R^2 on training data: 0.5173357362628234\n",
      "R^2 on test data: 0.5076580301932745\n",
      "#### wine_quality\n",
      "best params: {'alpha': 0.0001}\n",
      "best score: -0.5401265191014878\n",
      "MSE on training data: 0.5348196387905403\n",
      "MSE on test data: 0.5434554548609962\n",
      "R^2 on training data: 0.29251728914027875\n",
      "R^2 on test data: 0.3100144852264417\n"
     ]
    }
   ],
   "source": [
    "for dsname in datasets:\n",
    "    print(\"####\", dsname)\n",
    "    rreg = Ridge()\n",
    "    params = {\"alpha\": [0.00001, 0.0001, 0.001, 0.01, 0.1, 1., 2.5, 5., 10., 25., 50., 100., 250., 500., 1000., 2500., 5000., 10000., 25000., 50000., 100000.]}\n",
    "    rreg = test_model(dsname, rreg, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### diabetes\n",
      "best params: {'C': 10.0}\n",
      "best score: -3057.7070015538065\n",
      "MSE on training data: 2577.0841482085507\n",
      "MSE on test data: 3437.6800004513143\n",
      "R^2 on training data: 0.5804681274187382\n",
      "R^2 on test data: 0.32010692168648935\n",
      "#### boston\n",
      "best params: {'C': 100.0}\n",
      "best score: -13.598043246310898\n",
      "MSE on training data: 3.4469342608444284\n",
      "MSE on test data: 9.636188588435925\n",
      "R^2 on training data: 0.9594503764998732\n",
      "R^2 on test data: 0.8820688572255264\n",
      "#### concrete\n",
      "best params: {'C': 100.0}\n",
      "best score: -37.08377959823637\n",
      "MSE on training data: 18.997096173790347\n",
      "MSE on test data: 30.152373071635388\n",
      "R^2 on training data: 0.9333549806432162\n",
      "R^2 on test data: 0.8811781514521082\n",
      "#### airfoil\n",
      "best params: {'C': 250.0}\n",
      "best score: -7.094189398840056\n",
      "MSE on training data: 5.457762290823167\n",
      "MSE on test data: 7.477589784074904\n",
      "R^2 on training data: 0.8836028086716045\n",
      "R^2 on test data: 0.8511476320667879\n",
      "#### wine_quality\n",
      "best params: {'C': 10.0}\n",
      "best score: -0.4640006945371349\n",
      "MSE on training data: 0.32390678929749517\n",
      "MSE on test data: 0.4638085434435221\n",
      "R^2 on training data: 0.5715219922060317\n",
      "R^2 on test data: 0.4111363245289218\n"
     ]
    }
   ],
   "source": [
    "for dsname in datasets:\n",
    "    print(\"####\", dsname)\n",
    "    svr = SVR(gamma=\"scale\")\n",
    "    params = {\"C\": [1., 10., 25., 50., 100., 250.]}\n",
    "    svr = test_model(dsname, svr, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### diabetes\n",
      "best params: {'min_samples_leaf': 0.05}\n",
      "best score: -3336.6571277553485\n",
      "MSE on training data: 2472.319475907154\n",
      "MSE on test data: 3268.607555103584\n",
      "R^2 on training data: 0.5975231076301993\n",
      "R^2 on test data: 0.35354551553768243\n",
      "#### boston\n",
      "best params: {'min_samples_leaf': 0.0001}\n",
      "best score: -10.462810946604932\n",
      "MSE on training data: 1.4186988960396048\n",
      "MSE on test data: 10.583239343137262\n",
      "R^2 on training data: 0.9833104719321342\n",
      "R^2 on test data: 0.8704785093673091\n",
      "#### concrete\n",
      "best params: {'min_samples_leaf': 0.0001}\n",
      "best score: -28.701648337050347\n",
      "MSE on training data: 4.169688233206215\n",
      "MSE on test data: 27.527437198114896\n",
      "R^2 on training data: 0.9853720299949222\n",
      "R^2 on test data: 0.8915222703733743\n",
      "#### airfoil\n",
      "best params: {'min_samples_leaf': 0.0001}\n",
      "best score: -3.770457737476125\n",
      "MSE on training data: 0.4389576916890236\n",
      "MSE on test data: 3.316904702700349\n",
      "R^2 on training data: 0.9906383899294207\n",
      "R^2 on test data: 0.9339721576787678\n",
      "#### wine_quality\n",
      "best params: {'min_samples_leaf': 0.0001}\n",
      "best score: -0.3930715087362109\n",
      "MSE on training data: 0.05242462959399654\n",
      "MSE on test data: 0.3478176153846154\n",
      "R^2 on training data: 0.9306504167557255\n",
      "R^2 on test data: 0.558401495004132\n"
     ]
    }
   ],
   "source": [
    "for dsname in datasets:\n",
    "    print(\"####\", dsname)\n",
    "    rforest = RandomForestRegressor(n_estimators=100, random_state=13)\n",
    "    params = {\"min_samples_leaf\": [0.0001, 0.001, 0.01, 0.05, 0.1, 0.2]}\n",
    "    rforest = test_model(dsname, rforest, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### diabetes\n",
      "[AutoFeat] The 1 step feature engineering process could generate up to 70 features.\n",
      "[AutoFeat] With 353 data points this new feature matrix would use about 0.00 gb of space.\n",
      "[feateng] Step 1: transformation of original features\n",
      "[feateng] Generated 47 transformed features from 10 original features - done.\n",
      "[feateng] Removing correlated features, as well as additions at the highest level\n",
      "[feateng] Generated a total of 36 additional features\n",
      "[featsel] Scaling data...done.\n",
      "[featsel] Feature selection run 1/5\n",
      "[featsel] Feature selection run 2/5\n",
      "[featsel] Feature selection run 3/5\n",
      "[featsel] Feature selection run 4/5\n",
      "[featsel] Feature selection run 5/5\n",
      "[featsel] 5 features after 5 feature selection runs\n",
      "[featsel] 5 features after noise filtering\n",
      "[AutoFeat] Computing 1 new features.\n",
      "[AutoFeat]     1/    1 new features ...done.\n",
      "[AutoFeat] Final dataframe with 11 feature columns (1 new).\n",
      "[AutoFeat] Training final regression model.\n",
      "[AutoFeat] Trained model: largest coefficients:\n",
      "150.1322521573043\n",
      "1428.638808 * x009**2\n",
      "568.387384 * x002\n",
      "472.432217 * x008\n",
      "289.168818 * x003\n",
      "-193.923562 * x006\n",
      "[AutoFeat] Final score: 0.5263\n",
      "[AutoFeat] Computing 1 new features.\n",
      "[AutoFeat]     1/    1 new features ...done.\n",
      "autofeat new features: 1\n",
      "autofeat MSE on training data: 2910.0486534161682\n",
      "autofeat MSE on test data: 3198.3436910560895\n",
      "autofeat R^2 on training data: 0.5262637575420503\n",
      "autofeat R^2 on test data: 0.36744207217331515\n",
      "# Ridge Regression\n",
      "best params: {'alpha': 0.001}\n",
      "best score: -3025.6961292188253\n",
      "MSE on training data: 2751.8883426150587\n",
      "MSE on test data: 3085.3931185222505\n",
      "R^2 on training data: 0.5520111866295131\n",
      "R^2 on test data: 0.3897810660433738\n",
      "# Random Forest\n",
      "best params: {'min_samples_leaf': 0.05}\n",
      "best score: -3345.9299403661803\n",
      "MSE on training data: 2458.651902646058\n",
      "MSE on test data: 3257.3971754548543\n",
      "R^2 on training data: 0.599748096134302\n",
      "R^2 on test data: 0.3557626676656983\n",
      "#### boston\n",
      "[AutoFeat] The 1 step feature engineering process could generate up to 91 features.\n",
      "[AutoFeat] With 404 data points this new feature matrix would use about 0.00 gb of space.\n",
      "[feateng] Step 1: transformation of original features\n",
      "[feateng] Generated 60 transformed features from 13 original features - done.\n",
      "[feateng] Removing correlated features, as well as additions at the highest level\n",
      "[feateng] Generated a total of 22 additional features\n",
      "[featsel] Scaling data...done.\n",
      "[featsel] Feature selection run 1/5\n",
      "[featsel] Feature selection run 2/5\n",
      "[featsel] Feature selection run 3/5\n",
      "[featsel] Feature selection run 4/5\n",
      "[featsel] Feature selection run 5/5\n",
      "[featsel] 15 features after 5 feature selection runs\n",
      "[featsel] 11 features after noise filtering\n",
      "[AutoFeat] Computing 4 new features.\n",
      "[AutoFeat]     4/    4 new features ...done.\n",
      "[AutoFeat] Final dataframe with 17 feature columns (4 new).\n",
      "[AutoFeat] Training final regression model.\n",
      "[AutoFeat] Trained model: largest coefficients:\n",
      "37.56273372428286\n",
      "37.254085 * 1/x012\n",
      "23.533453 * 1/x007\n",
      "-22.815423 * x004\n",
      "2.039983 * 1/x002\n",
      "0.606209 * x005\n",
      "-0.603872 * x010\n",
      "-0.395622 * x012\n",
      "0.356939 * x008\n",
      "-0.214305 * x000\n",
      "-0.017932 * x009\n",
      "0.002618 * exp(x005)\n",
      "[AutoFeat] Final score: 0.8279\n",
      "[AutoFeat] Computing 4 new features.\n",
      "[AutoFeat]     4/    4 new features ...done.\n",
      "autofeat new features: 4\n",
      "autofeat MSE on training data: 14.631528677544708\n",
      "autofeat MSE on test data: 16.998861627698865\n",
      "autofeat R^2 on training data: 0.8278751684227363\n",
      "autofeat R^2 on test data: 0.7919618156886796\n",
      "# Ridge Regression\n",
      "best params: {'alpha': 1e-05}\n",
      "best score: -17.193604111578143\n",
      "MSE on training data: 14.110521589456532\n",
      "MSE on test data: 16.084328528462173\n",
      "R^2 on training data: 0.8340042790074267\n",
      "R^2 on test data: 0.8031542007803842\n",
      "# Random Forest\n",
      "best params: {'min_samples_leaf': 0.0001}\n",
      "best score: -10.234297920401229\n",
      "MSE on training data: 1.4080250792079214\n",
      "MSE on test data: 11.05366150000001\n",
      "R^2 on training data: 0.9834360383691708\n",
      "R^2 on test data: 0.864721313766983\n",
      "#### concrete\n",
      "[AutoFeat] The 1 step feature engineering process could generate up to 56 features.\n",
      "[AutoFeat] With 824 data points this new feature matrix would use about 0.00 gb of space.\n",
      "[feateng] Step 1: transformation of original features\n",
      "[feateng] Generated 34 transformed features from 8 original features - done.\n",
      "[feateng] Removing correlated features, as well as additions at the highest level\n",
      "[feateng] Generated a total of 12 additional features\n",
      "[featsel] Scaling data...done.\n",
      "[featsel] Feature selection run 1/5\n",
      "[featsel] Feature selection run 2/5\n",
      "[featsel] Feature selection run 3/5\n",
      "[featsel] Feature selection run 4/5\n",
      "[featsel] Feature selection run 5/5\n",
      "[featsel] 14 features after 5 feature selection runs\n",
      "[featsel] 13 features after noise filtering\n",
      "[AutoFeat] Computing 5 new features.\n",
      "[AutoFeat]     5/    5 new features ...done.\n",
      "[AutoFeat] Final dataframe with 13 feature columns (5 new).\n",
      "[AutoFeat] Training final regression model.\n",
      "[AutoFeat] Trained model: largest coefficients:\n",
      "-77.44463990138402\n",
      "9.864472 * log(x007)\n",
      "1.173751 * x004\n",
      "0.134299 * x000\n",
      "0.132340 * x001\n",
      "-0.130369 * x003\n",
      "-0.096499 * x004**2\n",
      "0.088637 * x002\n",
      "0.032595 * x006\n",
      "0.030263 * x005\n",
      "-0.028710 * x007\n",
      "0.001701 * x004**3\n",
      "[AutoFeat] Final score: 0.8486\n",
      "[AutoFeat] Computing 5 new features.\n",
      "[AutoFeat]     5/    5 new features ...done.\n",
      "autofeat new features: 5\n",
      "autofeat MSE on training data: 43.170654450409856\n",
      "autofeat MSE on test data: 45.0748341685121\n",
      "autofeat R^2 on training data: 0.8485500586420129\n",
      "autofeat R^2 on test data: 0.8223730150138463\n",
      "# Ridge Regression\n",
      "best params: {'alpha': 1e-05}\n",
      "best score: -45.09348662231908\n",
      "MSE on training data: 43.170654450409835\n",
      "MSE on test data: 45.07483416851206\n",
      "R^2 on training data: 0.8485500586420129\n",
      "R^2 on test data: 0.8223730150138464\n",
      "# Random Forest\n",
      "best params: {'min_samples_leaf': 0.0001}\n",
      "best score: -28.763036646076056\n",
      "MSE on training data: 4.146357453151285\n",
      "MSE on test data: 27.750414903359736\n",
      "R^2 on training data: 0.9854538783086932\n",
      "R^2 on test data: 0.8906435792315789\n",
      "#### airfoil\n",
      "[AutoFeat] Applying the Pi Theorem\n",
      "[AutoFeat] Pi Theorem 1:  x001 * x003 / x004\n",
      "[AutoFeat] The 1 step feature engineering process could generate up to 35 features.\n",
      "[AutoFeat] With 1202 data points this new feature matrix would use about 0.00 gb of space.\n",
      "[feateng] Step 1: transformation of original features\n",
      "[feateng] Generated 21 transformed features from 5 original features - done.\n",
      "[feateng] Removing correlated features, as well as additions at the highest level\n",
      "[feateng] Generated a total of 12 additional features\n",
      "[featsel] Scaling data...done.\n",
      "[featsel] Feature selection run 1/5\n",
      "[featsel] Feature selection run 2/5\n",
      "[featsel] Feature selection run 3/5\n",
      "[featsel] Feature selection run 4/5\n",
      "[featsel] Feature selection run 5/5\n",
      "[featsel] 10 features after 5 feature selection runs\n",
      "[featsel] 10 features after noise filtering\n",
      "[AutoFeat] Computing 5 new features.\n",
      "[AutoFeat]     5/    5 new features ...done.\n",
      "[AutoFeat] Final dataframe with 11 feature columns (6 new).\n",
      "[AutoFeat] Training final regression model.\n",
      "[AutoFeat] Trained model: largest coefficients:\n",
      "163.31467479772263\n",
      "-2213.716132 * 1/x000\n",
      "-628.350654 * x004**3\n",
      "-180.762017 * x004\n",
      "-22.190642 * x002\n",
      "-4.795042 * log(x000)\n",
      "0.092144 * x003\n",
      "0.003712 * 1/x004\n",
      "-0.000587 * x001\n",
      "-0.000373 * x000\n",
      "-0.000322 * x001**3\n",
      "[AutoFeat] Final score: 0.5645\n",
      "[AutoFeat] Applying the Pi Theorem\n",
      "[AutoFeat] Pi Theorem 1:  x001 * x003 / x004\n",
      "[AutoFeat] Computing 5 new features.\n",
      "[AutoFeat]     5/    5 new features ...done.\n",
      "autofeat new features: 5\n",
      "autofeat MSE on training data: 20.420425408778037\n",
      "autofeat MSE on test data: 22.579749525503157\n",
      "autofeat R^2 on training data: 0.5644954769632751\n",
      "autofeat R^2 on test data: 0.5505170407491426\n",
      "# Ridge Regression\n",
      "best params: {'alpha': 1e-05}\n",
      "best score: -21.20934670160552\n",
      "MSE on training data: 20.420429186903718\n",
      "MSE on test data: 22.57903418698184\n",
      "R^2 on training data: 0.5644953963875383\n",
      "R^2 on test data: 0.5505312806092909\n",
      "# Random Forest\n",
      "best params: {'min_samples_leaf': 0.0001}\n",
      "best score: -3.909768601966237\n",
      "MSE on training data: 0.4447366349374455\n",
      "MSE on test data: 3.413967939621606\n",
      "R^2 on training data: 0.9905151429415309\n",
      "R^2 on test data: 0.9320399718980286\n",
      "#### wine_quality\n",
      "[AutoFeat] The 1 step feature engineering process could generate up to 84 features.\n",
      "[AutoFeat] With 5197 data points this new feature matrix would use about 0.00 gb of space.\n",
      "[feateng] Step 1: transformation of original features\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[feateng] Generated 60 transformed features from 12 original features - done.\n",
      "[feateng] Removing correlated features, as well as additions at the highest level\n",
      "[feateng] Generated a total of 21 additional features\n",
      "[featsel] Scaling data...done.\n",
      "[featsel] Feature selection run 1/5\n",
      "[featsel] Feature selection run 2/5\n",
      "[featsel] Feature selection run 3/5\n",
      "[featsel] Feature selection run 4/5\n",
      "[featsel] Feature selection run 5/5\n",
      "[featsel] 15 features after 5 feature selection runs\n",
      "[featsel] 14 features after noise filtering\n",
      "[AutoFeat] Computing 8 new features.\n",
      "[AutoFeat]     8/    8 new features ...done.\n",
      "[AutoFeat] Final dataframe with 20 feature columns (8 new).\n",
      "[AutoFeat] Training final regression model.\n",
      "[AutoFeat] Trained model: largest coefficients:\n",
      "1.7194048045304786\n",
      "4.858025 * 1/x006\n",
      "-2.052140 * 1/x005\n",
      "-0.608784 * x001\n",
      "0.562178 * x009\n",
      "0.318012 * x010\n",
      "-0.301558 * x001**3\n",
      "-0.274285 * 1/x003\n",
      "0.200288 * x011\n",
      "-0.124830 * log(x004)\n",
      "0.086306 * 1/x001\n",
      "0.009013 * x003\n",
      "0.004119 * x005\n",
      "[AutoFeat] Final score: 0.3061\n",
      "[AutoFeat] Computing 8 new features.\n",
      "[AutoFeat]     8/    8 new features ...done.\n",
      "autofeat new features: 8\n",
      "autofeat MSE on training data: 0.5245592286418422\n",
      "autofeat MSE on test data: 0.523097023945766\n",
      "autofeat R^2 on training data: 0.30609020655024777\n",
      "autofeat R^2 on test data: 0.3358620911514204\n",
      "# Ridge Regression\n",
      "best params: {'alpha': 1e-05}\n",
      "best score: -0.5530021950238175\n",
      "MSE on training data: 0.5202044130912455\n",
      "MSE on test data: 0.5235536788123348\n",
      "R^2 on training data: 0.31185094622315446\n",
      "R^2 on test data: 0.3352823099745731\n",
      "# Random Forest\n",
      "best params: {'min_samples_leaf': 0.0001}\n",
      "best score: -0.39443729088620716\n",
      "MSE on training data: 0.052508081585530116\n",
      "MSE on test data: 0.3472515384615385\n",
      "R^2 on training data: 0.930540022826792\n",
      "R^2 on test data: 0.5591202013372399\n"
     ]
    }
   ],
   "source": [
    "for dsname in datasets:\n",
    "    print(\"####\", dsname)\n",
    "    test_autofeat(dsname, feateng_steps=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### diabetes\n",
      "[AutoFeat] The 2 step feature engineering process could generate up to 2485 features.\n",
      "[AutoFeat] With 353 data points this new feature matrix would use about 0.00 gb of space.\n",
      "[feateng] Step 1: transformation of original features\n",
      "[feateng] Generated 47 transformed features from 10 original features - done.\n",
      "[feateng] Step 2: first combination of features\n",
      "[feateng] Generated 6213 feature combinations from 1596 original feature tuples - done.\n",
      "[feateng] Removing correlated features, as well as additions at the highest level\n",
      "[feateng] Generated a total of 978 additional features\n",
      "[featsel] Scaling data...done.\n",
      "[featsel] Feature selection run 1/5\n",
      "[featsel] Feature selection run 2/5\n",
      "[featsel] Feature selection run 3/5\n",
      "[featsel] Feature selection run 4/5\n",
      "[featsel] Feature selection run 5/5\n",
      "[featsel] 5 features after 5 feature selection runs\n",
      "[featsel] 5 features after noise filtering\n",
      "[AutoFeat] Computing 5 new features.\n",
      "[AutoFeat]     5/    5 new features ...done.\n",
      "[AutoFeat] Final dataframe with 15 feature columns (5 new).\n",
      "[AutoFeat] Training final regression model.\n",
      "[AutoFeat] Trained model: largest coefficients:\n",
      "-402.38678504577126\n",
      "6604.507531 * x000**3*x001\n",
      "-4321.969329 * x006*Abs(x009)\n",
      "283.281255 * exp(x002)*exp(x003)\n",
      "270.765181 * exp(x002)*exp(x008)\n",
      "12.606497 * Abs(x008)/x008\n",
      "[AutoFeat] Final score: 0.5326\n",
      "[AutoFeat] Computing 5 new features.\n",
      "[AutoFeat]     5/    5 new features ...done.\n",
      "autofeat new features: 5\n",
      "autofeat MSE on training data: 2871.264448423081\n",
      "autofeat MSE on test data: 3217.089028130514\n",
      "autofeat R^2 on training data: 0.5325775638485446\n",
      "autofeat R^2 on test data: 0.36373468087282124\n",
      "# Ridge Regression\n",
      "best params: {'alpha': 0.1}\n",
      "best score: -2988.63450079624\n",
      "MSE on training data: 2757.4084187216918\n",
      "MSE on test data: 3056.995912487774\n",
      "R^2 on training data: 0.5511125555671876\n",
      "R^2 on test data: 0.3953973723382437\n",
      "# Random Forest\n",
      "best params: {'min_samples_leaf': 0.05}\n",
      "best score: -3033.4280112992046\n",
      "MSE on training data: 2162.9142796153774\n",
      "MSE on test data: 3577.479301652141\n",
      "R^2 on training data: 0.6478921813280434\n",
      "R^2 on test data: 0.292457874297835\n",
      "#### boston\n",
      "[AutoFeat] The 2 step feature engineering process could generate up to 4186 features.\n",
      "[AutoFeat] With 404 data points this new feature matrix would use about 0.01 gb of space.\n",
      "[feateng] Step 1: transformation of original features\n",
      "[feateng] Generated 60 transformed features from 13 original features - done.\n",
      "[feateng] Step 2: first combination of features\n",
      "[feateng] Generated 10381 feature combinations from 2628 original feature tuples - done.\n",
      "[feateng] Removing correlated features, as well as additions at the highest level\n",
      "[feateng] Generated a total of 1190 additional features\n",
      "[featsel] Scaling data...done.\n",
      "[featsel] Feature selection run 1/5\n",
      "[featsel] Feature selection run 2/5\n",
      "[featsel] Feature selection run 3/5\n",
      "[featsel] Feature selection run 4/5\n",
      "[featsel] Feature selection run 5/5\n",
      "[featsel] 17 features after 5 feature selection runs\n",
      "[featsel] 15 features after noise filtering\n",
      "[AutoFeat] Computing 14 new features.\n",
      "[AutoFeat]    14/   14 new features ...done.\n",
      "[AutoFeat] Final dataframe with 27 feature columns (14 new).\n",
      "[AutoFeat] Training final regression model.\n",
      "[AutoFeat] Trained model: largest coefficients:\n",
      "7.83348978245504\n",
      "-22.393277 * log(x004)/x007\n",
      "19.423286 * 1/(x007*x012)\n",
      "5.274188 * x005**3/x009\n",
      "3.082270 * x005**2/x010\n",
      "2.811826 * x008/x012\n",
      "0.232524 * exp(x005)/x009\n",
      "-0.097516 * x000\n",
      "0.030414 * x000**3*x003\n",
      "-0.021322 * sqrt(x000)*x012\n",
      "0.002793 * exp(x005)/x002\n",
      "-0.001003 * x005**3*x012\n",
      "-0.000699 * x006**2/x008\n",
      "-0.000043 * x010**3*sqrt(x012)\n",
      "0.000017 * x005**3*x011\n",
      "[AutoFeat] Final score: 0.9048\n",
      "[AutoFeat] Computing 14 new features.\n",
      "[AutoFeat]    14/   14 new features ...done.\n",
      "autofeat new features: 14\n",
      "autofeat MSE on training data: 8.09447899849645\n",
      "autofeat MSE on test data: 15.366787297485676\n",
      "autofeat R^2 on training data: 0.904776810063588\n",
      "autofeat R^2 on test data: 0.8119357285161958\n",
      "# Ridge Regression\n",
      "best params: {'alpha': 0.1}\n",
      "best score: -12.135098928784013\n",
      "MSE on training data: 7.631032110509982\n",
      "MSE on test data: 17.116806856325333\n",
      "R^2 on training data: 0.9102287843102779\n",
      "R^2 on test data: 0.7905183595473783\n",
      "# Random Forest\n",
      "best params: {'min_samples_leaf': 0.0001}\n",
      "best score: -10.884814079074072\n",
      "MSE on training data: 1.452285829207918\n",
      "MSE on test data: 10.257922617647054\n",
      "R^2 on training data: 0.9829153563333338\n",
      "R^2 on test data: 0.8744598524936517\n",
      "#### concrete\n",
      "[AutoFeat] The 2 step feature engineering process could generate up to 1596 features.\n",
      "[AutoFeat] With 824 data points this new feature matrix would use about 0.01 gb of space.\n",
      "[feateng] Step 1: transformation of original features\n",
      "[feateng] Generated 34 transformed features from 8 original features - done.\n",
      "[feateng] Step 2: first combination of features\n",
      "[feateng] Generated 3272 feature combinations from 861 original feature tuples - done.\n",
      "[feateng] Removing correlated features, as well as additions at the highest level\n",
      "[feateng] Generated a total of 349 additional features\n",
      "[featsel] Scaling data...done.\n",
      "[featsel] Feature selection run 1/5\n",
      "[featsel] Feature selection run 2/5\n",
      "[featsel] Feature selection run 3/5\n",
      "[featsel] Feature selection run 4/5\n",
      "[featsel] Feature selection run 5/5\n",
      "[featsel] 84 features after 5 feature selection runs\n",
      "[featsel] 44 features after noise filtering\n",
      "[AutoFeat] Computing 44 new features.\n",
      "[AutoFeat]    44/   44 new features ...done.\n",
      "[AutoFeat] Final dataframe with 52 feature columns (44 new).\n",
      "[AutoFeat] Training final regression model.\n",
      "[AutoFeat] Trained model: largest coefficients:\n",
      "-379.3703346555229\n",
      "2743.959896 * 1/(x000*x007)\n",
      "-740.646381 * sqrt(x004)/x000\n",
      "-495.337609 * log(x007)/x000\n",
      "74.158780 * x003/x005\n",
      "-32.850701 * x005/x006\n",
      "10.818919 * log(x003)*log(x006)\n",
      "1.807251 * sqrt(x001)/x000\n",
      "-0.455226 * x004/x007\n",
      "0.245898 * sqrt(x005)*log(x000)\n",
      "0.147783 * sqrt(x006)*log(x000)\n",
      "0.133477 * sqrt(x002)*sqrt(x007)\n",
      "0.089526 * sqrt(x001)*log(x007)\n",
      "0.083993 * x007**2/x000\n",
      "-0.043100 * x000/x007\n",
      "0.024712 * x001*log(x007)\n",
      "-0.011038 * sqrt(x001)*x002\n",
      "0.009774 * x006*log(x007)\n",
      "0.004285 * x001*sqrt(x002)\n",
      "-0.000164 * sqrt(x002)*x004**3\n",
      "0.000149 * x003**3/x000\n",
      "-0.000099 * x004**3*log(x007)\n",
      "[AutoFeat] Final score: 0.9152\n",
      "[AutoFeat] Computing 44 new features.\n",
      "[AutoFeat]    44/   44 new features ...done.\n",
      "autofeat new features: 44\n",
      "autofeat MSE on training data: 24.163471617739273\n",
      "autofeat MSE on test data: 34.23968494091808\n",
      "autofeat R^2 on training data: 0.9152304637003886\n",
      "autofeat R^2 on test data: 0.865071228433277\n",
      "# Ridge Regression\n",
      "best params: {'alpha': 0.001}\n",
      "best score: -28.846838561442233\n",
      "MSE on training data: 24.14428093499826\n",
      "MSE on test data: 33.4725540275912\n",
      "R^2 on training data: 0.9152977878540932\n",
      "R^2 on test data: 0.868094271196219\n",
      "# Random Forest\n",
      "best params: {'min_samples_leaf': 0.0001}\n",
      "best score: -27.49742600908869\n",
      "MSE on training data: 4.366416147082788\n",
      "MSE on test data: 30.93919395884354\n",
      "R^2 on training data: 0.9846818752729384\n",
      "R^2 on test data: 0.8780775161531202\n",
      "#### airfoil\n",
      "[AutoFeat] Applying the Pi Theorem\n",
      "[AutoFeat] Pi Theorem 1:  x001 * x003 / x004\n",
      "[AutoFeat] The 2 step feature engineering process could generate up to 630 features.\n",
      "[AutoFeat] With 1202 data points this new feature matrix would use about 0.00 gb of space.\n",
      "[feateng] Step 1: transformation of original features\n",
      "[feateng] Generated 21 transformed features from 5 original features - done.\n",
      "[feateng] Step 2: first combination of features\n",
      "[feateng] Generated 484 feature combinations from 325 original feature tuples - done.\n",
      "[feateng] Removing correlated features, as well as additions at the highest level\n",
      "[feateng] Generated a total of 201 additional features\n",
      "[featsel] Scaling data...done.\n",
      "[featsel] Feature selection run 1/5\n",
      "[featsel] Feature selection run 2/5\n",
      "[featsel] Feature selection run 3/5\n",
      "[featsel] Feature selection run 4/5\n",
      "[featsel] Feature selection run 5/5\n",
      "[featsel] 98 features after 5 feature selection runs\n",
      "[featsel] 89 features after noise filtering\n",
      "[AutoFeat] Computing 86 new features.\n",
      "[AutoFeat]    86/   86 new features ...done.\n",
      "[AutoFeat] Final dataframe with 92 feature columns (87 new).\n",
      "[AutoFeat] Training final regression model.\n",
      "[AutoFeat] Trained model: largest coefficients:\n",
      "52.94296476545382\n",
      "-2427.147373 * sqrt(x001)/x000\n",
      "2192.070730 * x001/x000\n",
      "-1973.367894 * x002*x004\n",
      "1800.175105 * sqrt(x000)*x004**3\n",
      "-925.724247 * x002/x000\n",
      "765.687194 * sqrt(x004)/x003\n",
      "655.420922 * log(x002)/x003\n",
      "614.041331 * x002/x003\n",
      "-409.460177 * x002**2*x004\n",
      "311.339447 * log(x002)/x000\n",
      "-215.881674 * x004\n",
      "-161.005355 * x003/x000\n",
      "-154.245915 * sqrt(x002)*log(x002)\n",
      "-149.970594 * x001*x002**2\n",
      "143.348142 * sqrt(x001)*x002**2\n",
      "-113.921559 * x001**2/x000\n",
      "-112.975012 * 1/(x000*x003)\n",
      "69.689918 * x001**2*x002**3\n",
      "-66.684402 * x002**3/x000\n",
      "-62.441622 * x004/x000\n",
      "-49.856731 * sqrt(x001)*x002**3\n",
      "-46.220808 * x004**3\n",
      "36.783363 * x002**3/x003\n",
      "35.039991 * x001**3*x004**3\n",
      "30.488137 * x003**2*x004**3\n",
      "-28.376489 * x002**2*x004**2\n",
      "27.932352 * 1/(x002*x003)\n",
      "22.487478 * sqrt(x001)/x003\n",
      "-17.376659 * 1/(x000*x002)\n",
      "14.953334 * sqrt(x000)/x003\n",
      "-11.646334 * sqrt(x000)*sqrt(x004)\n",
      "-11.396960 * x000*x004**2\n",
      "10.084446 * x001**2*x002**2\n",
      "-7.409161 * x001**3*x002**3\n",
      "5.858735 * x002**2*x003\n",
      "4.612033 * x003*sqrt(x004)\n",
      "-3.306584 * log(x000)*log(x002)\n",
      "3.288982 * 1/(x000*x004)\n",
      "-2.851992 * sqrt(x000)*sqrt(x002)\n",
      "-2.829460 * x001**3*x004**2\n",
      "2.007307 * x001**3/x000\n",
      "1.797826 * sqrt(x003)*log(x002)\n",
      "1.600107 * x000*x004\n",
      "-1.205045 * sqrt(x004)/x002\n",
      "1.006624 * sqrt(x003)*log(x000)\n",
      "-0.994044 * x003\n",
      "0.785336 * x002**3/x004\n",
      "0.142804 * x001**3/x003\n",
      "-0.113284 * x000*sqrt(x004)\n",
      "-0.108350 * x002/x004\n",
      "-0.047510 * x000*x002**3\n",
      "0.047165 * x000*x002\n",
      "-0.038468 * sqrt(x002)/x004\n",
      "-0.031971 * sqrt(x000)*x001\n",
      "0.025563 * x000**2*x004**3\n",
      "0.021489 * x003**3/x000\n",
      "0.009377 * sqrt(x000)/x002\n",
      "-0.001920 * x000**2*x004**2\n",
      "-0.001665 * x003**3*x004\n",
      "-0.001527 * x002**3*x003**3\n",
      "0.001237 * x000*log(x002)\n",
      "0.000845 * x000*sqrt(x001)\n",
      "-0.000584 * x001**2/x004\n",
      "-0.000227 * 1/(x002*x004)\n",
      "-0.000042 * x001**3/x002\n",
      "-0.000028 * x000/x002\n",
      "0.000024 * sqrt(x000)*x001**3\n",
      "0.000016 * x001**2*x003**2\n",
      "-0.000015 * x000*x001**2\n",
      "-0.000011 * sqrt(x001)*x003**3\n",
      "[AutoFeat] Final score: 0.9021\n",
      "[AutoFeat] Applying the Pi Theorem\n",
      "[AutoFeat] Pi Theorem 1:  x001 * x003 / x004\n",
      "[AutoFeat] Computing 86 new features.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[AutoFeat]    86/   86 new features ...done.\n",
      "autofeat new features: 86\n",
      "autofeat MSE on training data: 4.589137314006273\n",
      "autofeat MSE on test data: 6.456693041512904\n",
      "autofeat R^2 on training data: 0.9021278931717449\n",
      "autofeat R^2 on test data: 0.8714700757864599\n",
      "# Ridge Regression\n",
      "best params: {'alpha': 1e-05}\n",
      "best score: -5.963166271148654\n",
      "MSE on training data: 4.558225857595683\n",
      "MSE on test data: 6.514198642157381\n",
      "R^2 on training data: 0.9027871389421429\n",
      "R^2 on test data: 0.8703253426475035\n",
      "# Random Forest\n",
      "best params: {'min_samples_leaf': 0.0001}\n",
      "best score: -3.658552968640786\n",
      "MSE on training data: 0.440488213986107\n",
      "MSE on test data: 3.0536668747960265\n",
      "R^2 on training data: 0.9906057486220216\n",
      "R^2 on test data: 0.939212291885729\n",
      "#### wine_quality\n",
      "[AutoFeat] The 2 step feature engineering process could generate up to 3570 features.\n",
      "[AutoFeat] With 5197 data points this new feature matrix would use about 0.07 gb of space.\n",
      "[feateng] Step 1: transformation of original features\n",
      "[feateng] Generated 60 transformed features from 12 original features - done.\n",
      "[feateng] Step 2: first combination of features\n",
      "[feateng] Generated 10160 feature combinations from 2556 original feature tuples - done.\n",
      "[feateng] Removing correlated features, as well as additions at the highest level\n",
      "[feateng] Generated a total of 1088 additional features\n",
      "[featsel] Scaling data...done.\n",
      "[featsel] Feature selection run 1/5\n",
      "[featsel] Feature selection run 2/5\n",
      "[featsel] Feature selection run 3/5\n",
      "[featsel] Feature selection run 4/5\n",
      "[featsel] Feature selection run 5/5\n",
      "[featsel] 27 features after 5 feature selection runs\n",
      "[featsel] 24 features after noise filtering\n",
      "[AutoFeat] Computing 24 new features.\n",
      "[AutoFeat]    24/   24 new features ...done.\n",
      "[AutoFeat] Final dataframe with 36 feature columns (24 new).\n",
      "[AutoFeat] Training final regression model.\n",
      "[AutoFeat] Trained model: largest coefficients:\n",
      "-1417.2444654277413\n",
      "523.414269 * exp(x007)/x007\n",
      "-34.602375 * x002*log(x007)\n",
      "-12.190273 * sqrt(x005)*log(x007)\n",
      "9.740262 * x009**2/x006\n",
      "-8.516813 * x004**2*log(x009)\n",
      "-8.341963 * sqrt(x001)/x000\n",
      "3.295997 * log(x009)/x006\n",
      "1.409286 * x011/x005\n",
      "0.540502 * x001**3*log(x009)\n",
      "0.428386 * log(x009)/x005\n",
      "0.249561 * log(x009)/x003\n",
      "-0.205654 * x002**3*log(x003)\n",
      "-0.102590 * 1/(x004*x005)\n",
      "-0.035544 * sqrt(x004)*exp(x008)\n",
      "-0.029203 * sqrt(x004)*sqrt(x006)\n",
      "0.011538 * x003/x001\n",
      "0.002688 * x000**2*x011\n",
      "0.000461 * x000**3*log(x009)\n",
      "0.000374 * sqrt(x001)*x010**3\n",
      "0.000343 * exp(x008)/x004\n",
      "0.000096 * x005**2*log(x009)\n",
      "0.000035 * x010**3*log(x005)\n",
      "-0.000013 * x006**2*x009**2\n",
      "0.000011 * x010**3*exp(x008)\n",
      "[AutoFeat] Final score: 0.3559\n",
      "[AutoFeat] Computing 24 new features.\n",
      "[AutoFeat]    24/   24 new features ...done.\n",
      "autofeat new features: 24\n",
      "autofeat MSE on training data: 0.48691070536565095\n",
      "autofeat MSE on test data: 0.4937011746987263\n",
      "autofeat R^2 on training data: 0.3558933128227474\n",
      "autofeat R^2 on test data: 0.373183844008078\n",
      "# Ridge Regression\n",
      "best params: {'alpha': 0.01}\n",
      "best score: -0.4945409186600737\n",
      "MSE on training data: 0.48601225625655436\n",
      "MSE on test data: 0.4968708002829054\n",
      "R^2 on training data: 0.3570818204338565\n",
      "R^2 on test data: 0.36915960297640293\n",
      "# Random Forest\n",
      "best params: {'min_samples_leaf': 0.0001}\n",
      "best score: -0.3928513145220997\n",
      "MSE on training data: 0.05165914950933229\n",
      "MSE on test data: 0.34542661538461544\n",
      "R^2 on training data: 0.931663027150198\n",
      "R^2 on test data: 0.5614371722635414\n"
     ]
    }
   ],
   "source": [
    "for dsname in datasets:\n",
    "    print(\"####\", dsname)\n",
    "    test_autofeat(dsname, feateng_steps=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### diabetes\n",
      "[AutoFeat] The 3 step feature engineering process could generate up to 60445 features.\n",
      "[AutoFeat] With 353 data points this new feature matrix would use about 0.09 gb of space.\n",
      "[feateng] Step 1: transformation of original features\n",
      "[feateng] Generated 47 transformed features from 10 original features - done.\n",
      "[feateng] Step 2: first combination of features\n",
      "[feateng] Generated 6213 feature combinations from 1596 original feature tuples - done.\n",
      "[feateng] Step 3: transformation of new features\n",
      "[feateng] Generated 25952 transformed features from 6213 original features - done.\n",
      "[feateng] Removing correlated features, as well as additions at the highest level\n",
      "[feateng] Generated a total of 15536 additional features\n",
      "[featsel] Scaling data...done.\n",
      "[featsel] Feature selection run 1/5\n",
      "[featsel] Feature selection run 2/5\n",
      "[featsel] Feature selection run 3/5\n",
      "[featsel] Feature selection run 4/5\n",
      "[featsel] Feature selection run 5/5\n",
      "[featsel] 12 features after 5 feature selection runs\n",
      "[featsel] 8 features after noise filtering\n",
      "[AutoFeat] Computing 8 new features.\n",
      "[AutoFeat]     8/    8 new features ...done.\n",
      "[AutoFeat] Final dataframe with 18 feature columns (8 new).\n",
      "[AutoFeat] Training final regression model.\n",
      "[AutoFeat] Trained model: largest coefficients:\n",
      "-225.8854988082355\n",
      "-3772.658600 * exp(x006)*Abs(x001)\n",
      "-2743.736151 * (x001 + x006)**3\n",
      "642.795027 * x000**3*x001\n",
      "365.950289 * exp(x002)*exp(x003)\n",
      "231.071492 * Abs(x006 - Abs(x009))\n",
      "177.495728 * exp(x002)*exp(x008)\n",
      "11.162150 * Abs(x008)/x008\n",
      "1.201335 * x008/Abs(x002)\n",
      "[AutoFeat] Final score: 0.5686\n",
      "[AutoFeat] Computing 8 new features.\n",
      "[AutoFeat]     8/    8 new features ...done.\n",
      "autofeat new features: 8\n",
      "autofeat MSE on training data: 2649.8938224790145\n",
      "autofeat MSE on test data: 3116.584703144686\n",
      "autofeat R^2 on training data: 0.5686152048007658\n",
      "autofeat R^2 on test data: 0.3836120967141634\n",
      "# Ridge Regression\n",
      "best params: {'alpha': 0.1}\n",
      "best score: -2865.9704368803878\n",
      "MSE on training data: 2629.0258058315544\n",
      "MSE on test data: 3087.2070106055544\n",
      "R^2 on training data: 0.5720123767973618\n",
      "R^2 on test data: 0.3894223204149023\n",
      "# Random Forest\n",
      "best params: {'min_samples_leaf': 0.05}\n",
      "best score: -2973.9674104175365\n",
      "MSE on training data: 2122.61359087621\n",
      "MSE on test data: 3643.2680106690545\n",
      "R^2 on training data: 0.6544528609336396\n",
      "R^2 on test data: 0.2794463991500875\n",
      "#### boston\n",
      "[AutoFeat] The 3 step feature engineering process could generate up to 102466 features.\n",
      "[AutoFeat] With 404 data points this new feature matrix would use about 0.17 gb of space.\n",
      "[feateng] Step 1: transformation of original features\n",
      "[feateng] Generated 60 transformed features from 13 original features - done.\n",
      "[feateng] Step 2: first combination of features\n",
      "[feateng] Generated 10381 feature combinations from 2628 original feature tuples - done.\n",
      "[feateng] Step 3: transformation of new features\n",
      "[feateng] Generated 42017 transformed features from 10381 original features - done.\n",
      "[feateng] Removing correlated features, as well as additions at the highest level\n",
      "[feateng] Generated a total of 24242 additional features\n",
      "[featsel] Scaling data...done.\n",
      "[featsel] Feature selection run 1/5\n",
      "[featsel] Feature selection run 2/5\n",
      "[featsel] Feature selection run 3/5\n",
      "[featsel] Feature selection run 4/5\n",
      "[featsel] Feature selection run 5/5\n",
      "[featsel] 26 features after 5 feature selection runs\n",
      "[featsel] 19 features after noise filtering\n",
      "[AutoFeat] Computing 19 new features.\n",
      "[AutoFeat]    19/   19 new features ...done.\n",
      "[AutoFeat] Final dataframe with 32 feature columns (19 new).\n",
      "[AutoFeat] Training final regression model.\n",
      "[AutoFeat] Trained model: largest coefficients:\n",
      "15.690301353995709\n",
      "297.340104 * x005**6/x009**3\n",
      "20.539692 * 1/(sqrt(x012) + log(x007))\n",
      "13.704522 * 1/(x012*log(x007))\n",
      "4.720594 * 1/(-sqrt(x006) + x012**2)\n",
      "-4.654507 * x004**6*log(x000)**2\n",
      "2.514438 * 1/(sqrt(x012) + 1/x008)\n",
      "1.281135 * x005**3/x009\n",
      "-0.807630 * 1/(x007 + x012)\n",
      "0.661434 * Abs(x005 - log(x009))\n",
      "-0.371804 * log(x010**3*x012)\n",
      "-0.090246 * 1/(log(x007) - 1/x005)\n",
      "0.081982 * x005**6/x010**3\n",
      "-0.058550 * 1/(-x003 + 1/x012)\n",
      "0.010869 * (-sqrt(x009) + x012)**2\n",
      "-0.000675 * x002**(3/2)*log(x012)**3\n",
      "-0.000472 * exp(x005)*log(x000)\n",
      "0.000344 * exp(-x000 + x005)\n",
      "[AutoFeat] Final score: 0.9135\n",
      "[AutoFeat] Computing 19 new features.\n",
      "[AutoFeat]    19/   19 new features ...done.\n",
      "autofeat new features: 19\n",
      "autofeat MSE on training data: 7.355445228607461\n",
      "autofeat MSE on test data: 90.17211890263076\n",
      "autofeat R^2 on training data: 0.9134707795028358\n",
      "autofeat R^2 on test data: -0.10355883251855325\n",
      "# Ridge Regression\n",
      "best params: {'alpha': 0.001}\n",
      "best score: -10.116744610958419\n",
      "MSE on training data: 6.106282207628637\n",
      "MSE on test data: 85.63612020021495\n",
      "R^2 on training data: 0.9281658930030752\n",
      "R^2 on test data: -0.048045648473838076\n",
      "# Random Forest\n",
      "best params: {'min_samples_leaf': 0.0001}\n",
      "best score: -11.549821463827154\n",
      "MSE on training data: 1.371444190594059\n",
      "MSE on test data: 11.47755465686274\n",
      "R^2 on training data: 0.9838663747632943\n",
      "R^2 on test data: 0.8595335568084805\n",
      "#### concrete\n",
      "[AutoFeat] The 3 step feature engineering process could generate up to 38556 features.\n",
      "[AutoFeat] With 824 data points this new feature matrix would use about 0.13 gb of space.\n",
      "[feateng] Step 1: transformation of original features\n",
      "[feateng] Generated 34 transformed features from 8 original features - done.\n",
      "[feateng] Step 2: first combination of features\n",
      "[feateng] Generated 3272 feature combinations from 861 original feature tuples - done.\n",
      "[feateng] Step 3: transformation of new features\n",
      "[feateng] Generated 9450 transformed features from 3272 original features - done.\n",
      "[feateng] Removing correlated features, as well as additions at the highest level\n",
      "[feateng] Generated a total of 4421 additional features\n",
      "[featsel] Scaling data...done.\n",
      "[featsel] Feature selection run 1/5\n",
      "[featsel] Feature selection run 2/5\n",
      "[featsel] Feature selection run 3/5\n",
      "[featsel] Feature selection run 4/5\n",
      "[featsel] Feature selection run 5/5\n",
      "[featsel] 20 features after 5 feature selection runs\n",
      "[featsel] 18 features after noise filtering\n",
      "[AutoFeat] Computing 18 new features.\n",
      "[AutoFeat]    18/   18 new features ...done.\n",
      "[AutoFeat] Final dataframe with 26 feature columns (18 new).\n",
      "[AutoFeat] Training final regression model.\n",
      "[AutoFeat] Trained model: largest coefficients:\n",
      "-8.609766317950537\n",
      "-3403.427427 * exp(-sqrt(x000) + sqrt(x004))\n",
      "-2659.190721 * 1/(x000 + x001)\n",
      "598.148390 * log(x007)/x003\n",
      "132.161686 * sqrt(x000)/x003\n",
      "-17.364956 * 1/(sqrt(x003) - x004**3)\n",
      "-13.147368 * 1/(sqrt(x002) + x007)\n",
      "1.342888 * log(x000**3*x007)\n",
      "0.441939 * sqrt(sqrt(x004)*x007)\n",
      "0.121225 * sqrt(x001)*log(x007)\n",
      "-0.014123 * Abs(x003 - x004**2)\n",
      "-0.002375 * (sqrt(x003) - sqrt(x007))**3\n",
      "0.000380 * x001*log(x007)**2\n",
      "-0.000087 * (-x003 + x007)**2\n",
      "[AutoFeat] Final score: 0.8623\n",
      "[AutoFeat] Computing 18 new features.\n",
      "[AutoFeat]    18/   18 new features ...done.\n",
      "autofeat new features: 18\n",
      "autofeat MSE on training data: 39.243067617757944\n",
      "autofeat MSE on test data: 43.95921073452956\n",
      "autofeat R^2 on training data: 0.862328696076542\n",
      "autofeat R^2 on test data: 0.8267693667833809\n",
      "# Ridge Regression\n",
      "best params: {'alpha': 1e-05}\n",
      "best score: -39.26505429021526\n",
      "MSE on training data: 36.143736584736466\n",
      "MSE on test data: 42.96120849332761\n",
      "R^2 on training data: 0.8732016723882465\n",
      "R^2 on test data: 0.8307022071894353\n",
      "# Random Forest\n",
      "best params: {'min_samples_leaf': 0.0001}\n",
      "best score: -26.08424973978551\n",
      "MSE on training data: 4.183665621392929\n",
      "MSE on test data: 25.768748038947212\n",
      "R^2 on training data: 0.9853229949583178\n",
      "R^2 on test data: 0.8984527596060783\n",
      "#### airfoil\n",
      "[AutoFeat] Applying the Pi Theorem\n",
      "[AutoFeat] Pi Theorem 1:  x001 * x003 / x004\n",
      "[AutoFeat] The 3 step feature engineering process could generate up to 14910 features.\n",
      "[AutoFeat] With 1202 data points this new feature matrix would use about 0.07 gb of space.\n",
      "[feateng] Step 1: transformation of original features\n",
      "[feateng] Generated 21 transformed features from 5 original features - done.\n",
      "[feateng] Step 2: first combination of features\n",
      "[feateng] Generated 484 feature combinations from 325 original feature tuples - done.\n",
      "[feateng] Step 3: transformation of new features\n",
      "[feateng] Generated 1610 transformed features from 484 original features - done.\n",
      "[feateng] Removing correlated features, as well as additions at the highest level\n",
      "[feateng] Generated a total of 1057 additional features\n",
      "[featsel] Scaling data...done.\n",
      "[featsel] Feature selection run 1/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[featsel] Feature selection run 2/5\n",
      "[featsel] Feature selection run 3/5\n",
      "[featsel] Feature selection run 4/5\n",
      "[featsel] Feature selection run 5/5\n",
      "[featsel] 17 features after 5 feature selection runs\n",
      "[featsel] 14 features after noise filtering\n",
      "[AutoFeat] Computing 14 new features.\n",
      "[AutoFeat]    14/   14 new features ...done.\n",
      "[AutoFeat] Final dataframe with 20 feature columns (15 new).\n",
      "[AutoFeat] Training final regression model.\n",
      "[AutoFeat] Trained model: largest coefficients:\n",
      "144.24608738841482\n",
      "-2080.040265 * x001**2*x004**4\n",
      "-2010.967809 * 1/(x000**2*x004)\n",
      "-939.267704 * 1/(x003**2*log(x002))\n",
      "-733.183950 * sqrt(x004)/x003\n",
      "-423.569496 * sqrt(x002)/x003\n",
      "240.944001 * 1/(x000**2*x002**2)\n",
      "-3.730616 * 1/(x000*x002**2)\n",
      "-2.505675 * 1/(x000**2*x004**2)\n",
      "-1.640789 * sqrt(x000)*sqrt(x004)\n",
      "-0.732964 * sqrt(x000)*sqrt(x002)\n",
      "0.570508 * sqrt(x000)*x002\n",
      "0.012427 * x000*sqrt(x004)\n",
      "0.002037 * 1/(x002**2 + 1/x000)\n",
      "-0.000247 * x000*sqrt(x001)\n",
      "[AutoFeat] Final score: 0.8103\n",
      "[AutoFeat] Applying the Pi Theorem\n",
      "[AutoFeat] Pi Theorem 1:  x001 * x003 / x004\n",
      "[AutoFeat] Computing 14 new features.\n",
      "[AutoFeat]    14/   14 new features ...done.\n",
      "autofeat new features: 14\n",
      "autofeat MSE on training data: 8.895978714431841\n",
      "autofeat MSE on test data: 10.39454851725712\n",
      "autofeat R^2 on training data: 0.8102762851694513\n",
      "autofeat R^2 on test data: 0.7930812995805705\n",
      "# Ridge Regression\n",
      "best params: {'alpha': 1e-05}\n",
      "best score: -9.394101241466362\n",
      "MSE on training data: 8.80427742069166\n",
      "MSE on test data: 10.146041798420702\n",
      "R^2 on training data: 0.8122319901752344\n",
      "R^2 on test data: 0.7980281895028947\n",
      "# Random Forest\n",
      "best params: {'min_samples_leaf': 0.0001}\n",
      "best score: -3.7946726115517095\n",
      "MSE on training data: 0.47025341066680554\n",
      "MSE on test data: 3.2819746348415664\n",
      "R^2 on training data: 0.9899709490268109\n",
      "R^2 on test data: 0.9346674917988503\n",
      "#### wine_quality\n",
      "[AutoFeat] The 3 step feature engineering process could generate up to 87234 features.\n",
      "[AutoFeat] With 5197 data points this new feature matrix would use about 1.81 gb of space.\n",
      "[feateng] Step 1: transformation of original features\n",
      "[feateng] Generated 60 transformed features from 12 original features - done.\n",
      "[feateng] Step 2: first combination of features\n",
      "[feateng] Generated 10160 feature combinations from 2556 original feature tuples - done.\n",
      "[feateng] Step 3: transformation of new features\n",
      "[feateng] Generated 45284 transformed features from 10160 original features - done.\n",
      "[feateng] Removing correlated features, as well as additions at the highest level\n",
      "[feateng] Generated a total of 23749 additional features\n",
      "[featsel] Scaling data...done.\n",
      "[featsel] Feature selection run 1/5\n",
      "[featsel] Feature selection run 2/5\n",
      "[featsel] Feature selection run 3/5\n",
      "[featsel] Feature selection run 4/5\n",
      "[featsel] Feature selection run 5/5\n",
      "[featsel] 31 features after 5 feature selection runs\n",
      "[featsel] 24 features after noise filtering\n",
      "[AutoFeat] Computing 24 new features.\n",
      "[AutoFeat]    24/   24 new features ...done.\n",
      "[AutoFeat] Final dataframe with 36 feature columns (24 new).\n",
      "[AutoFeat] Training final regression model.\n",
      "[AutoFeat] Trained model: largest coefficients:\n",
      "5.9310980124278325\n",
      "0.770519 * x008**6*log(x007)**2\n",
      "0.725093 * exp(-x001)/log(x006)\n",
      "-0.495155 * x001**2*log(x009)**2\n",
      "0.406920 * x009**2/x006\n",
      "0.305629 * 1/(x001**2 - sqrt(x010))\n",
      "-0.237512 * log(x001*sqrt(x006))\n",
      "-0.227857 * (-x002 + 1/x003)**2\n",
      "-0.207618 * (-exp(x011) + log(x000))**2\n",
      "-0.129066 * Abs(log(x004) + log(x005))\n",
      "-0.118429 * log(x010**3/x001)\n",
      "-0.106140 * exp(x003**2*log(x001))\n",
      "-0.046764 * log(x009)**2/x003**2\n",
      "-0.031848 * 1/(x005*x009**3)\n",
      "-0.011538 * 1/(x003**3 + x010**3)\n",
      "-0.004578 * (-log(x005) + log(x006))**3\n",
      "-0.004077 * 1/(-x001**3 + log(x007))\n",
      "-0.003736 * exp(-x000**2 + x008**2)\n",
      "0.003161 * sqrt(x009)*x010**2\n",
      "-0.001809 * x001/x010**3\n",
      "0.000191 * x010**3*log(x005)\n",
      "0.000071 * sqrt(x003)*x010**3\n",
      "0.000052 * x010**3*exp(x009)\n",
      "[AutoFeat] Final score: 0.3402\n",
      "[AutoFeat] Computing 24 new features.\n",
      "[AutoFeat]    24/   24 new features ...done.\n",
      "autofeat new features: 24\n",
      "autofeat MSE on training data: 0.49877307313424946\n",
      "autofeat MSE on test data: 0.5120235391599125\n",
      "autofeat R^2 on training data: 0.34020125610410856\n",
      "autofeat R^2 on test data: 0.3499212822626009\n",
      "# Ridge Regression\n",
      "best params: {'alpha': 1e-05}\n",
      "best score: -0.5039116134705615\n",
      "MSE on training data: 0.4809723931326292\n",
      "MSE on test data: 70.36209948956375\n",
      "R^2 on training data: 0.36374877087221347\n",
      "R^2 on test data: -88.33359487443671\n",
      "# Random Forest\n",
      "best params: {'min_samples_leaf': 0.0001}\n",
      "best score: -0.391646311153476\n",
      "MSE on training data: 0.05138381758706946\n",
      "MSE on test data: 0.3477782307692308\n",
      "R^2 on training data: 0.9320272482083275\n",
      "R^2 on test data: 0.5584514987604238\n"
     ]
    }
   ],
   "source": [
    "for dsname in datasets:\n",
    "    print(\"####\", dsname)\n",
    "    test_autofeat(dsname, feateng_steps=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
